import numpy as np
from gaussianprojection import *

print(tf.__version__)
#tf.disable_eager_execution()
print('Dimension Extraction with KLD from actual distribution')
# Parameters
r = 10
d = 100
reduce_r = 2
noise_var = 1
large_mu = True  # else Small Mu
noise_cov = noise_var*np.identity(d)

# Previously, s_mu, s_sigma, and H were generated by the function calls below
H = generate_rand_mat(r, d)
s_mu, s_sigma = gen_gaussian_n_dim(r, 1, 1)

# Read in the file belonging to the large/small mu scenario
if large_mu:
    file_path = '../data/dimensionExtractionLargeMu.pickle'
else:
    file_path = '../data/dimensionExtractionSmallMu.pickle'

with open(file_path, 'rb') as file:
    s_mu = pickle.load(file)
    s_sigma = pickle.load(file)
    H = pickle.load(file)

# Assign our distribution.
s_1 = [s_mu[0], s_sigma[0]]
s_2 = [s_mu[1], s_sigma[1]]

#Calculate the KLD for s
s_kld = gaussian_kld(s_1, s_2)
print('S KLD: ' + str(s_kld.flatten()[0]))

# Calculate the distributions for X where we define
# x = Hs + z
# and z~N(0,\sigma^2)
x_mu_list = list()
x_sigma_list = list()
for dist in range(2):
    x_mu_list.append(np.matmul(H, s_mu[dist]))
    signal_cov = np.matmul(H, np.matmul(s_sigma[dist], H.transpose()))
    x_sigma_list.append(signal_cov + noise_cov)
x_mu = np.array(x_mu_list)
x_sigma = np.array(x_sigma_list)
x_1 = [x_mu[0], x_sigma[0]]
x_2 = [x_mu[1], x_sigma[1]]

# Calculate the KLD for x
x_kld = gaussian_kld(x_1, x_2)
print('X KLD: ' + str(x_kld.flatten()[0]))

# Get the mean and covariance contributions to the KLD of the X distributions.
# We do this to check that we are actually in a large or small mu regime
mu_kld = gaussian_kld_mu(x_1, x_2)
sigma_kld = gaussian_kld_sigma(x_1, x_2)
print('X Mean KLD Contribution: ' + str(mu_kld.flatten()[0]))
print('X Covariance KLD Contribution: ' + str(sigma_kld))

# Find the A matrix projection using Algorithm 1
A_1 = algorithm1(x_mu, x_sigma, reduce_r)[1]

# Calculate the distributions for Y where
# y = Ax
# and A is found as in Algorithm 1
y1_mu_list = list()
y1_sigma_list = list()
for dist in range(2):
    y1_mu_list.append(np.matmul(A_1, x_mu[dist]))
    signal_cov = np.matmul(A_1, np.matmul(x_sigma[dist], A_1.transpose()))
    y1_sigma_list.append(signal_cov)
y1_mu = np.array(y1_mu_list)
y1_sigma = np.array(y1_sigma_list)
y1_1 = [y1_mu[0], y1_sigma[0]]
y1_2 = [y1_mu[1], y1_sigma[1]]

# Calculate the KLD for y under Algorithm 1
y1_kld = gaussian_kld(y1_1, y1_2)
print('Y KLD under Algorithm 1 (r = 2): ' + str(y1_kld.flatten()[0]))

# Find the A matrix projection using Algorithm 2
# We must first diagonalize the distributions and
# then construct "A" using the r eigenvectors that
# maximize the KLD.
diag_mean, diag_var, whitening = diagonalize_h1(x_mu, x_sigma)
A_2 = algorithm2(diag_mean[1], diag_var[1], reduce_r)[1]

# Calculate the distributions for Y where
# y = Ax
# and A is found as in Algorithm 2
y2_mu_list = list()
y2_sigma_list = list()
for dist in range(2):
    y2_mu_list.append(np.matmul(A_2, diag_mean[dist]))
    signal_cov = np.matmul(A_2, np.matmul(diag_var[dist], A_2.transpose()))
    y2_sigma_list.append(signal_cov)
y2_mu = np.array(y2_mu_list)
y2_sigma = np.array(y2_sigma_list)
y2_1 = [y2_mu[0], y2_sigma[0]]
y2_2 = [y2_mu[1], y2_sigma[1]]

# Calculate the KLD for y under Algorithm 2
y2_kld = gaussian_kld(y2_1, y2_2)
print('Y KLD under Algorithm 2 (r = 2): ' + str(y2_kld.flatten()[0]))

# Get the reduced KLD for each r going from 1 to n to compare the
# performance of both Algorithms across r.
print('Plotting KLD over r')
reduced_kld_algorithm1 = list()
reduced_kld_algorithm2 = list()
for dim in range(1,r+1):
  reduced_kld_algorithm1.append(algorithm1(x_mu, x_sigma, dim)[0])
  reduced_kld_algorithm2.append(algorithm2(diag_mean[1], diag_var[1], dim)[0])

#print("Gradiant Ascent Search")
#s_mu, s_sigma = gen_gaussian_n_dim(5, 1, 1)
#s_sigma[1] = s_sigma[0] # equal covariance matrices
#target_kld = gaussian_kld([s_mu[0], s_sigma[0]], [s_mu[1], s_sigma[1]])
#mu_diff = np.subtract(s_mu[1], s_mu[0])
#grad_ascent_A = gradient_ascent(s_mu, s_sigma, 5, 1, np.eye(1,5))
#print("Target KLD")
#print(target_kld)

print("Gradient Ascent Search")
r = 2
gradient_ascent_pipe = Pipeline([('test', GradientAscent(r=reduce_r, num_steps=1000, attempts=1, init_val=np.eye(reduce_r, d), verbose=False)),('scaler', StandardScaler()), ('svc', SVC())])
grad_ascent_kld = gradient_ascent_pipe['test'].distribution_fit(x_mu, x_sigma)
print('Y KLD under Gradient Ascent (r = 2): ' + str(grad_ascent_kld.flatten()[0]))

gradient_ascent_pipe1 = Pipeline([('test', GradientAscent(r=reduce_r, num_steps=1000, attempts=1, init_val=A_1, verbose=False)),('scaler', StandardScaler()), ('svc', SVC())])
grad_ascent_kld_1 = gradient_ascent_pipe1['test'].distribution_fit(x_mu, x_sigma)
print('Y KLD under Gradient Ascent starting at A_1 (r = 2): ' + str(grad_ascent_kld_1.flatten()[0]))
grad_ascent_xform_1 = gradient_ascent_pipe1['test'].xform
concat_matrix_1 = np.vstack((A_1, grad_ascent_xform_1))
print('Matrix Rank under Gradient Ascent starting at A_1 (r = 2): ' + str(np.linalg.matrix_rank(concat_matrix_1)))
S_1 = np.linalg.svd(concat_matrix_1, compute_uv=False)
print('Singular Values: ' + str(sorted(S_1, reverse=True)))

A2_mod = np.matmul(A_2, whitening[1])
gradient_ascent_pipe2 = Pipeline([('test', GradientAscent(r=reduce_r, num_steps=1000, attempts=1, init_val=A2_mod, verbose=False)),('scaler', StandardScaler()), ('svc', SVC())])
grad_ascent_kld_2 = gradient_ascent_pipe2['test'].distribution_fit(x_mu, x_sigma)
print('Y KLD under Gradient Ascent starting at A_2 (r = 2): ' + str(grad_ascent_kld_2.flatten()[0]))
grad_ascent_xform_2 = gradient_ascent_pipe2['test'].xform
concat_matrix_2 = np.vstack((A_2, grad_ascent_xform_2))
print('Matrix Rank under Gradient Ascent starting at A_2 (r = 2): ' + str(np.linalg.matrix_rank(concat_matrix_2)))
S_2 = np.linalg.svd(concat_matrix_2, compute_uv=False)
print('Singular Values: ' + str(sorted(S_2, reverse=True)))
