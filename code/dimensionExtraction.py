from gaussianprojection import *

# Function to plot contours of the Gaussian distributions
# mu - list of means
# sigma - list of covariances
# axis_scale - scaling factor on which to calculate the pdf values
# text_title - title for the figure
def plot_contour(mu, sigma, axis_scale, text_title):
    pdf_list = list()
    # For each pdf we need to generate a meshgrid of values with which
    # to plot the contour lines.
    for i in range(len(mu)):
        mean = mu[i].flatten()
        cov = sigma[i]
        sigma_1, sigma_2 = cov[0, 0], cov[1, 1]

        # Generating a Gaussian bivariate distribution
        # with given mean and covariance matrix
        distr = scipy.stats.multivariate_normal(cov=cov, mean=mean)

        # Get 1/1000 the peak pdf value for the level curve to plot
        sigma_det = np.linalg.det(cov)
        contour_line = math.log(0.001*(1/(2*np.pi*np.sqrt(sigma_det))))

        # Make a meshgrid. We use a custom scaling for each distribution
        # since depending on the variance it may need to be bigger or smaller
        # to encapsulate the contour.
        x = np.linspace(-1 * axis_scale * sigma_1+mean[0], axis_scale * sigma_1+mean[0], num=1000)
        y = np.linspace(-1 * axis_scale * sigma_2+mean[1], axis_scale * sigma_2+mean[1], num=1000)
        X, Y = np.meshgrid(x, y)

        # Generating the density function
        # for each point in the meshgrid
        pdf = np.zeros(X.shape)
        for i in range(X.shape[0]):
            for j in range(X.shape[1]):
                pdf[i, j] = distr.logpdf([X[i, j], Y[i, j]])
        pdf_list.append((pdf, X, Y, contour_line))

    # Plotting contour plots
    plt.figure()
    pl1 = plt.contour(pdf_list[0][1], pdf_list[0][2], pdf_list[0][0], [pdf_list[0][3]], colors='blue', linewidths=3, linestyles='dashed')
    pl2 = plt.contour(pdf_list[1][1], pdf_list[1][2], pdf_list[1][0], [pdf_list[1][3]], colors='red', linewidths=3, linestyles='solid')
    plt.title(text_title, fontsize="20")
    plt.tight_layout()
    h1, _ = pl1.legend_elements()
    h2, _ = pl2.legend_elements()
    plt.legend([h1[0], h2[0]], ['Class 1', 'Class 2'], fontsize="16", handlelength=5, loc ="lower right")

# Use SVD to create a dxr matrix of rank r
# H = UPV where
# U is dxd and invertible
# P is a structure where the top rxr portion is
# identity and then the bottom d-r rows are all zero
# V is rxr and invertible
def generate_h(r, d):
    # Create a dxd invertible matrix
    U = np.random.rand(d, d)
    u_x = np.sum(np.abs(U), axis=1)
    np.fill_diagonal(U, u_x)

    # Create a rxr invertible matrix
    V = np.random.rand(r, r)
    v_x = np.sum(np.abs(V), axis=1)
    np.fill_diagonal(V, v_x)

    # Create P which is rank r
    P = np.identity(r)
    P_app = np.zeros([d-r, r])
    P = np.vstack((P, P_app))

    H = np.matmul(U, np.matmul(P, V))
    return H


#@title Dimension Extraction with KLD from actual distribution
print('Dimension Extraction with KLD from actual distribution')
# Parameters
r = 10
d = 100
reduce_r = 2
noise_var = 1
large_mu = True  # else Small Mu
noise_cov = noise_var*np.identity(d)

# Previously, s_mu, s_sigma, and H were generated by the function calls below
#H = generate_h(r, d)
#s_mu, s_sigma = gen_gaussian_n_dim(r, 1, 1)

# Read in the file belonging to the large/small mu scenario
if large_mu:
    file_path = 'dimensionExtractionLargeMu.pickle'
else:
    file_path = 'dimensionExtractionSmallMu.pickle'

with open(file_path, 'rb') as file:
    s_mu = pickle.load(file)
    s_sigma = pickle.load(file)
    H = pickle.load(file)

# Assign our distribution.
s_1 = [s_mu[0], s_sigma[0]]
s_2 = [s_mu[1], s_sigma[1]]

#Calculate the KLD for s
s_kld = gaussian_kld(s_1, s_2)
print('S KLD: ' + str(s_kld.flatten()[0]))

# Calculate the distributions for X where we define
# x = Hs + z
# and z~N(0,\sigma^2)
x_mu_list = list()
x_sigma_list = list()
for dist in range(2):
    x_mu_list.append(np.matmul(H, s_mu[dist]))
    signal_cov = np.matmul(H, np.matmul(s_sigma[dist], H.transpose()))
    x_sigma_list.append(signal_cov + noise_cov)
x_mu = np.array(x_mu_list)
x_sigma = np.array(x_sigma_list)
x_1 = [x_mu[0], x_sigma[0]]
x_2 = [x_mu[1], x_sigma[1]]

# Calculate the KLD for x
x_kld = gaussian_kld(x_1, x_2)
print('X KLD: ' + str(x_kld.flatten()[0]))

# Get the mean and covariance contributions to the KLD of the X distributions.
# We do this to check that we are actually in a large or small mu regime
mu_kld = gaussian_kld_mu(x_1, x_2)
sigma_kld = gaussian_kld_sigma(x_1, x_2)
print('X Mean KLD Contribution: ' + str(mu_kld.flatten()[0]))
print('X Covariance KLD Contribution: ' + str(sigma_kld))

# Find the A matrix projection using Algorithm 1
A_1 = algorithm1(x_mu, x_sigma, reduce_r)[1]

# Calculate the distributions for Y where
# y = Ax
# and A is found as in Algorithm 1
y1_mu_list = list()
y1_sigma_list = list()
for dist in range(2):
    y1_mu_list.append(np.matmul(A_1, x_mu[dist]))
    signal_cov = np.matmul(A_1, np.matmul(x_sigma[dist], A_1.transpose()))
    y1_sigma_list.append(signal_cov)
y1_mu = np.array(y1_mu_list)
y1_sigma = np.array(y1_sigma_list)
y1_1 = [y1_mu[0], y1_sigma[0]]
y1_2 = [y1_mu[1], y1_sigma[1]]

# Calculate the KLD for y under Algorithm 1
y1_kld = gaussian_kld(y1_1, y1_2)
print('Y KLD under Algorithm 1 (r = 2): ' + str(y1_kld.flatten()[0]))

# Find the A matrix projection using Algorithm 2
# We must first diagonalize the distributions and
# then construct "A" using the r eigenvectors that
# maximize the KLD.
diag_mean, diag_var, whitening = diagonalize_h1(x_mu, x_sigma)
A_2 = algorithm2(diag_mean[1], diag_var[1], reduce_r)[1]

# Calculate the distributions for Y where
# y = Ax
# and A is found as in Algorithm 2
y2_mu_list = list()
y2_sigma_list = list()
for dist in range(2):
    y2_mu_list.append(np.matmul(A_2, diag_mean[dist]))
    signal_cov = np.matmul(A_2, np.matmul(diag_var[dist], A_2.transpose()))
    y2_sigma_list.append(signal_cov)
y2_mu = np.array(y2_mu_list)
y2_sigma = np.array(y2_sigma_list)
y2_1 = [y2_mu[0], y2_sigma[0]]
y2_2 = [y2_mu[1], y2_sigma[1]]

# Calculate the KLD for y under Algorithm 2
y2_kld = gaussian_kld(y2_1, y2_2)
print('Y KLD under Algorithm 2 (r = 2): ' + str(y2_kld.flatten()[0]))

# Get the reduced KLD for each r going from 1 to n to compare the
# performance of both Algorithms across r.
print('Plotting KLD over r')
reduced_kld_algorithm1 = list()
reduced_kld_algorithm2 = list()
for dim in range(1,r+1):
  reduced_kld_algorithm1.append(algorithm1(x_mu, x_sigma, dim)[0])
  reduced_kld_algorithm2.append(algorithm2(diag_mean[1], diag_var[1], dim)[0])

plt.figure()
plt.scatter(range(1,r+1), reduced_kld_algorithm1, label="Algorithm 1")
plt.scatter(range(1,r+1), reduced_kld_algorithm2, label="Algorithm 2")
if large_mu:
    plt.title('KLD vs Dimensionality Large-Mu')
else:
    plt.title('KLD vs Dimensionality Small-Mu')
plt.xlabel('Number of Dimensions')
plt.ylabel('KLD')
plt.legend()

# Different scalings were needed depending on the distributions being plotted.
# These numbers where found empirically.
if large_mu:
    axis_scalings = [100, 8]
else:
    axis_scalings = [100, 5]

# Plot the reduced dimension for Algorithm 1.
# We must make sure to project into an orthanormal set of coordinates
# with QR decomposition
Q1, R1 = np.linalg.qr(A_1.transpose())
y1_mu_orthogonal = list()
y1_sigma_orthogonal = list()
for dist in range(2):
    y1_mu_orthogonal.append(np.matmul(Q1.transpose(), x_mu[dist]))
    y1_sigma_orthogonal.append(np.matmul(Q1.transpose(), np.matmul(x_sigma[dist], Q1)))

if large_mu:
    plot_contour(y1_mu_orthogonal, y1_sigma_orthogonal, axis_scalings[1], 'Algorithm 1, Large-$\mu$')
else:
    plot_contour(y1_mu_orthogonal, y1_sigma_orthogonal, axis_scalings[1], 'Algorithm 1, Small-$\mu$')

# Plot the reduced dimension for Algorithm 2.
# We must make sure to project into an orthanormal set of coordinates
# with QR decomposition. We must also make sure to take the whitened distributions
# for this algorithm.
Q2, R2 = np.linalg.qr(A_2.transpose())
y2_mu_orthogonal = list()
y2_sigma_orthogonal = list()
for dist in range(2):
    y2_mu_orthogonal.append(np.matmul(Q2.transpose(), diag_mean[dist]))
    y2_sigma_orthogonal.append(np.matmul(Q2.transpose(), np.matmul(diag_var[dist], Q2)))

if large_mu:
    plot_contour(y2_mu_orthogonal, y2_sigma_orthogonal, axis_scalings[0], 'Algorithm 2, Large-$\mu$')
else:
    plot_contour(y2_mu_orthogonal, y2_sigma_orthogonal, axis_scalings[0], 'Algorithm 2, Small-$\mu$')

plt.show()
