from gaussianprojection import *

# Function to plot contours of the Gaussian distributions
# mu - list of means
# sigma - list of covariances
# axis_scale - scaling factor on which to calculate the pdf values
# text_title - title for the figure
def plot_contour(mu, sigma, axis_scale, text_title):
    pdf_list = list()
    # For each pdf we need to generate a meshgrid of values with which
    # to plot the contour lines.
    for i in range(len(mu)):
        mean = mu[i].flatten()
        cov = sigma[i]
        sigma_1, sigma_2 = cov[0, 0], cov[1, 1]

        # Generating a Gaussian bivariate distribution
        # with given mean and covariance matrix
        distr = scipy.stats.multivariate_normal(cov=cov, mean=mean)

        # Get 1/1000 the peak pdf value for the level curve to plot
        sigma_det = np.linalg.det(cov)
        contour_line = math.log(0.001*(1/(2*np.pi*np.sqrt(sigma_det))))

        # Make a meshgrid. We use a custom scaling for each distribution
        # since depending on the variance it may need to be bigger or smaller
        # to encapsulate the contour.
        x = np.linspace(-1 * axis_scale * sigma_1+mean[0], axis_scale * sigma_1+mean[0], num=1000)
        y = np.linspace(-1 * axis_scale * sigma_2+mean[1], axis_scale * sigma_2+mean[1], num=1000)
        X, Y = np.meshgrid(x, y)

        # Generating the density function
        # for each point in the meshgrid
        pdf = np.zeros(X.shape)
        for i in range(X.shape[0]):
            for j in range(X.shape[1]):
                pdf[i, j] = distr.logpdf([X[i, j], Y[i, j]])
        pdf_list.append((pdf, X, Y, contour_line))

    # Plotting contour plots
    plt.figure()
    pl1 = plt.contour(pdf_list[0][1], pdf_list[0][2], pdf_list[0][0], [pdf_list[0][3]], colors='blue', linewidths=3, linestyles='dashed')
    pl2 = plt.contour(pdf_list[1][1], pdf_list[1][2], pdf_list[1][0], [pdf_list[1][3]], colors='red', linewidths=3, linestyles='solid')
    plt.title(text_title, fontsize="20")
    plt.tight_layout()
    h1, _ = pl1.legend_elements()
    h2, _ = pl2.legend_elements()
    plt.legend([h1[0], h2[0]], ['Class 1', 'Class 2'], fontsize="16", handlelength=5, loc ="lower right")

#@title Dimension Extraction with KLD from actual distribution
print('Dimension Extraction with KLD from actual distribution')
# Parameters
r = 10
d = 100
reduce_r = 2
noise_var = 1
num_steps = 1000
num_attempts = 1
large_mu = False  # else Small Mu
noise_cov = noise_var*np.identity(d)

# Previously, s_mu, s_sigma, and H were generated by the function calls below
#H = generate_h(r, d)
#s_mu, s_sigma = gen_gaussian_n_dim(r, 1, 1)

# Read in the file belonging to the large/small mu scenario
if large_mu:
    file_path = '../../../../data/dimensionExtractionLargeMu.pickle'
else:
    file_path = '../../../../data/dimensionExtractionSmallMu.pickle'

with open(file_path, 'rb') as file:
    s_mu = pickle.load(file)
    s_sigma = pickle.load(file)
    H = pickle.load(file)

# Assign our distribution.
s_1 = [s_mu[0], s_sigma[0]]
s_2 = [s_mu[1], s_sigma[1]]

#Calculate the KLD for s
s_kld = gaussian_kld(s_1, s_2)
print('S KLD: ' + str(s_kld.flatten()[0]))

# Calculate the distributions for X where we define
# x = Hs + z
# and z~N(0,\sigma^2)
x_mu_list = list()
x_sigma_list = list()
for dist in range(2):
    x_mu_list.append(np.matmul(H, s_mu[dist]))
    signal_cov = np.matmul(H, np.matmul(s_sigma[dist], H.transpose()))
    x_sigma_list.append(signal_cov + noise_cov)
x_mu = np.array(x_mu_list)
x_sigma = np.array(x_sigma_list)
x_1 = [x_mu[0], x_sigma[0]]
x_2 = [x_mu[1], x_sigma[1]]

# Calculate the KLD for x
x_kld = gaussian_kld(x_1, x_2)
print('X KLD: ' + str(x_kld.flatten()[0]))

# Get the mean and covariance contributions to the KLD of the X distributions.
# We do this to check that we are actually in a large or small mu regime
mu_kld = gaussian_kld_mu(x_1, x_2)
sigma_kld = gaussian_kld_sigma(x_1, x_2)
print('X Mean KLD Contribution: ' + str(mu_kld.flatten()[0]))
print('X Covariance KLD Contribution: ' + str(sigma_kld))

# Different scalings were needed depending on the distributions being plotted.
# These numbers where found empirically.
axis_scalings = defaultdict(int)
if large_mu:
    axis_scalings["Algorithm 1"] = 8
    axis_scalings["Algorithm 2"] = 100
    axis_scalings["Gradient Descent (Algorithm 1)"] = 8
    axis_scalings["Gradient Descent (Algorithm 2)"] = 8
else:
    axis_scalings["Algorithm 1"] = 8
    axis_scalings["Algorithm 2"] = 100
    axis_scalings["Gradient Descent (Algorithm 1)"] = 8
    axis_scalings["Gradient Descent (Algorithm 2)"] = 8

# Define our pipelines
stage = 'dimensionReduction'
algorithm1_pipe = Pipeline([(stage, Algorithm1(r=reduce_r, verbose=False)),('scaler', StandardScaler()), ('svc', SVC())])
algorithm2_pipe = Pipeline([(stage, Algorithm2(r=reduce_r, verbose=False)),('scaler', StandardScaler()), ('svc', SVC())])
gradient_ascent_pipe = Pipeline([(stage, GradientAscent(r=reduce_r, num_steps=num_steps, attempts=num_attempts, init_val=algorithm1_pipe[stage].full_xform, verbose=False)),('scaler', StandardScaler()), ('svc', SVC())])
gradient_ascent_pipe[stage].name = 'Gradient Descent (Algorithm 1)'
gradient_ascent_pipe2 = Pipeline([(stage, GradientAscent(r=reduce_r, num_steps=num_steps, attempts=num_attempts, init_val=algorithm2_pipe[stage].full_xform, verbose=False)),('scaler', StandardScaler()), ('svc', SVC())])
gradient_ascent_pipe2[stage].name = 'Gradient Descent (Algorithm 2)'
pipes = [algorithm1_pipe, gradient_ascent_pipe, algorithm2_pipe, gradient_ascent_pipe2]
#pipes = [gradient_ascent_pipe]
last_xform = None

for pipe in pipes:
    # If doing a Gradient ascent, use the last xform as the initial value
    if pipe[stage].name[:16] == 'Gradient Descent':
        print('Found Gradient Descent!')
        pipe[stage].reinit(r=reduce_r, init_val=last_xform)

    # Project to r = 2 and get the KLD
    print("Testing with " + pipe[stage].name + "...")
    new_kld = pipe[stage].distribution_fit(x_mu, x_sigma)
    print(pipe[stage].name + " KLD (r = " + str(reduce_r) + "): " + str(new_kld))

    # Plot the contour in the 2 dimensional space
    orth_means = pipe[stage].orth_means
    orth_covs = pipe[stage].orth_covs
    if large_mu:
        plot_contour(orth_means, orth_covs, axis_scalings[pipe[stage].name], pipe[stage].name + ', Large-$\mu$')
    else:
        plot_contour(orth_means, orth_covs, axis_scalings[pipe[stage].name], pipe[stage].name + ', Small-$\mu$')

    last_xform = pipe[stage].full_xform

class plotSettings(object):
    facecolors = None
    marker = None
    linestyle = None

alg1_settings = plotSettings()
alg1_settings.facecolors = 'none'
alg1_settings.marker = 'o'
alg1_settings.linestyle = 'dashed'

alg2_settings = plotSettings()
alg2_settings.facecolors = 'none'
alg2_settings.marker = 'o'
alg2_settings.linestyle = 'dotted'

alg1_gd_settings = plotSettings()
alg1_gd_settings.marker = 'x'
alg1_gd_settings.linestyle = 'dashed'

alg2_gd_settings = plotSettings()
alg2_gd_settings.marker = '+'
alg2_gd_settings.linestyle = 'dotted'

# Need a second loop for the KLD vs r curve since all pipelines are contributing to the same graph
print("Plotting KLD vs r curve")
last_xform_list = list(range(1, r+1))
marker_list = (alg1_settings, alg1_gd_settings, alg2_settings, alg2_gd_settings)
plt.figure()
all_kld_lists = list()
for pipe, mark in zip(pipes, marker_list):
    # Plot the KLD vs r curve
    kld_list = list()
    xform_list = list()
    for dim in range(1, r+1):
        #print(dim)
        # Reinitialize with the new dimension
        pipe[stage].reinit(dim, init_val=last_xform_list[dim-1])
        kld_list.append(pipe[stage].distribution_fit(x_mu, x_sigma))
        xform_list.append(pipe[stage].full_xform)
    plt.plot(range(1,r+1), kld_list, label=pipe[stage].name, mfc=mark.facecolors, marker=mark.marker, linestyle=mark.linestyle)
    last_xform_list = xform_list
    all_kld_lists.append(kld_list)
if large_mu:
    plt.title('KLD vs Dimensionality Large-Mu')
else:
    plt.title('KLD vs Dimensionality Small-Mu')
plt.xlabel('Number of Dimensions')
plt.ylabel('KLD')
plt.legend()

plt.show()